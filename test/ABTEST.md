# A/B Test Name 1: View A Selected User's Successful And Unsuccessful Trades

## User Story Number: 

US3, Project Item #7

## Metric: 

(Engagement, Signals)

## Hypothesis:

The problem we are trying to solve is the potential worry of a user not trusting trading process with another user and fears of an unsuccessful trade. Can a visible successful and unsuccessful trade rating result in a higher success trade percentage amongst all users?

## Experiment:

The impact of this would outcome to a potential lose of a current user not return back to our site. The given scenario affects the incoming and referring traffic of our application. We believe that the key information to gather from this hypothesized scenario is to maintain a statistic of the number of completed and incompleted trades of all users. The intent of this statistic is all users would agree to this information gathering, which would be supportive for future user communications through our application's messaging platform. Our experiment will be implemented by maintaining a record according to user feedback whether a trade was successful or not and a comment. This will provide full transparency on a user's trades percentage could prevent negative trades hurting our platform's traffic and encourage completed user-to-user trading.

## Variations:

We will be testing via our Thunkable display screens, where would be the best positions to display the public percentages of someone's successful and unsuccessful trades. This can be displayed either on the Profile Screen of a selected user or can be associated with their posting amongst other users through our application's trading feed.






#
---Below is a template for A/B Testing---
#

# A/B Test Name 2: Color and Layout Theme UniSwap


User Story Number:
US1, All


## Metric (from the HEART grid):
(Happiness, Retention)


## Hypothesis:


As of now this will only be available to students at the University of Rhode Island. Given that it is local and that we want the feel to be as legitimate as possible, utilizing school colors may provide additional trust as well as a more pleasant viewing experience. As a team, for moving forward with the app and improving user experience a strong color pallet will reflect professionalism, unity, and trust.


We believe that the application could either go neutral off white colors for a modern look, or alternatively colors that represent the local university. From this the layout and feel will then better be adjusted for development and the product overall will have a stronger imprint on the people who use it.


## Experiment


We will test between neutral modern colors vs the colors of the University of Rhode Island to measure how professional it looks, whether it seems appealing to others, and whether or not it looks good enough for people to hold trust.


## Variations


Neutrals: Whites, grays, and blacks are commonly used as background colors to create a clean and uncluttered look. They also provide a neutral canvas for other elements to stand out.


URI: Blue, light blue, white the main colors of the University of Rhode Island. Representing the local nature of the app as well as its headquarters.

# A/B Test Name

User Story Number:

## Metric (from the HEART grid):

## Hypothesis:
The key part of a A/B test is formulating your hypothesis as this basically guides the whole A/B test plan. What problem are we trying to solve? Its impact? (e.g. how big this problem is to our customers?) In formulating the hypothesis, first you need to define the problem you want to solve. For example, you are an SaaS that offers free trial and you want to improve the traffic-to-lead conversion ratio (i.e. attracting more website visitors to actually sign up for a free trial). But that problem might be too broad to form an A/B test as you can simply test one variable in an A/B test to be effective (otherwise you won’t know which variable is causing the change). So to narrow down the problem you want to solve, you need to find out the bottle-neck in the conversion funnel – where do people drop off the most? Are there any key information or call-to-action buttons that you expect people to read/click but they didn’t? 

After narrowing down the problem you want to solve, you then need to make a hypothesis as to what causes those bottlenecks and what you can do to improve. For example, you noticed most of the visitors will visit your “Features” page but very few of them will actually scroll past even half of the page so many features that you think are important are not actually viewed by the visitors. To improve this, one hypothesis might be using tab or toggle list design to make your page shorter and visitors can select to dig deeper into content that they are interested in by expanding the content.

Remember when formulating your hypothesis, change only one variable so that you will know it’s really that variable that is causing the change in conversion.

Now you have your hypothesis, the next is to plan how you are going to measure your results. Defining your success metrics carefully beforehand is important. Otherwise, if there is not enough tracking done during the experiment, it might be hard to draw conclusions and next steps at the end of the experiment.

## Experiment

Detail out the experiment setup that you will use to test your hypothesis using Firebase capabilities. Describe the audiences – will all users be able to view the experiment? Or you will only allocate x% of your user base to the experiment? Lay out the details with the rationale behind this decision. Describe the tracking using Firebase Analytics. With the success metrics that you have defined, what tracking needs to be set up? 

## Variations

In this section, describe what variations you would like to test. Layout the design work related and add diagrams, mockups and designs related to the confirmed variation that you’d like to test.
